---
title: "Decision Tree Challenge"
subtitle: "Feature Importance and Categorical Variable Encoding"
format:
  html: default
  pdf: default
execute:
  echo: false
  eval: true
  warning: false
---

# ðŸŒ³ Decision Tree Challenge - Feature Importance and Variable Encoding


## The Ames Housing Dataset ðŸ 

We are analyzing the Ames Housing dataset which contains detailed information about residential properties sold in Ames, Iowa from 2006 to 2010. This dataset is perfect for our analysis because it contains a categorical variable (like zip code) and numerical variables (like square footage, year built, number of bedrooms).

## The Problem: ZipCode as Numerical vs Categorical

**Key Question:** What happens when we treat zipCode as a numerical variable in a decision tree? How does this affect feature importance interpretation?

**The Issue:** Zip codes (50010, 50011, 50012, 50013) are categorical variables representing discrete geographic areas, i.e. neighborhoods. When treated as numerical, the tree might split on "zipCode > 50012.5" - which has no meaningful interpretation for house prices.  Zip codes are non-ordinal categorical variables meaning they have no inherent order that aids house price prediction (i.e. zip code 99999 is not the priceiest zip code).

## Data Loading and Model Building


```{python}
#| label: load-and-model-python
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')
warnings.simplefilter('ignore')
import os
os.environ['PYTHONWARNINGS'] = 'ignore'

# Load data
sales_data = pd.read_csv("salesPriceData.csv")

# Prepare model data (treating zipCode as numerical)
model_vars = ['SalePrice', 'LotArea', 'YearBuilt', 'GrLivArea', 'FullBath', 
              'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'zipCode']
model_data = sales_data[model_vars].dropna()

# Split data
X = model_data.drop('SalePrice', axis=1)
y = model_data['SalePrice']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# Build decision tree
tree_model = DecisionTreeRegressor(max_depth=3, 
                                  min_samples_split=20, 
                                  min_samples_leaf=10, 
                                  random_state=123)
tree_model.fit(X_train, y_train)

print(f"Model built with {tree_model.get_n_leaves()} terminal nodes")
```

## Tree Visualization


```{python}
#| label: visualize-tree-python
#| echo: false
#| fig-width: 10
#| fig-height: 6

# Visualize tree
plt.figure(figsize=(10, 6))
plot_tree(tree_model, 
          feature_names=X_train.columns,
          filled=True, 
          rounded=True,
          fontsize=10,
          max_depth=3)
plt.title("Decision Tree (zipCode as Numerical)")
plt.tight_layout()
plt.show()
```

:::

## Feature Importance Analysis


```{python}
#| label: feature-importance-python
#| echo: false

# Extract and display feature importance
importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': tree_model.feature_importances_
}).sort_values('Importance', ascending=False)

importance_df['Importance_Percent'] = (importance_df['Importance'] * 100).round(2)

# Check zipCode ranking
zipcode_rank = importance_df[importance_df['Feature'] == 'zipCode'].index[0] + 1
zipcode_importance = importance_df[importance_df['Feature'] == 'zipCode']['Importance_Percent'].iloc[0]
```

```{python}
#| label: importance-plot-python
#| echo: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance
plt.figure(figsize=(8, 5))
plt.barh(range(len(importance_df)), importance_df['Importance'], 
         color='steelblue', alpha=0.7)
plt.yticks(range(len(importance_df)), importance_df['Feature'])
plt.xlabel('Importance Score')
plt.title('Feature Importance (zipCode as Numerical)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```



## Proper Categorical Encoding: The Solution


**Python Approach:** One-hot encode zipCode (create dummy variables for each zip code)

### Categorical Encoding Analysis



```{python}
#| label: categorical-python
#| echo: false
#| include: false
# One-hot encode zipCode
import pandas as pd

# Create one-hot encoded zipCode
zipcode_encoded = pd.get_dummies(model_data['zipCode'], prefix='zipCode')
model_data_cat = pd.concat([model_data.drop('zipCode', axis=1), zipcode_encoded], axis=1)

# Split data
X_cat = model_data_cat.drop('SalePrice', axis=1)
y_cat = model_data_cat['SalePrice']
X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y_cat, test_size=0.2, random_state=123)

# Build decision tree with one-hot encoded zipCode
tree_model_cat = DecisionTreeRegressor(max_depth=3, 
                                      min_samples_split=20, 
                                      min_samples_leaf=10, 
                                      random_state=123)
tree_model_cat.fit(X_train_cat, y_train_cat)

# Feature importance with one-hot encoded zipCode
importance_cat_df = pd.DataFrame({
    'Feature': X_train_cat.columns,
    'Importance': tree_model_cat.feature_importances_
}).sort_values('Importance', ascending=False)

importance_cat_df['Importance_Percent'] = (importance_cat_df['Importance'] * 100).round(2)

# Check zipCode features
zipcode_features = [col for col in X_train_cat.columns if col.startswith('zipCode')]
zipcode_importance = importance_cat_df[importance_cat_df['Feature'].isin(zipcode_features)]['Importance'].sum()
total_importance = importance_cat_df['Importance'].sum()
zipcode_percent = (zipcode_importance / total_importance * 100).round(2)
```

### Tree Visualization: Categorical zipCode


```{python}
#| label: visualize-tree-cat-python
#| echo: false
#| fig-width: 10
#| fig-height: 6

# Visualize tree with one-hot encoded zipCode
plt.figure(figsize=(10, 6))
plot_tree(tree_model_cat, 
          feature_names=X_train_cat.columns,
          filled=True, 
          rounded=True,
          fontsize=8,
          max_depth=4)
plt.title("Decision Tree (zipCode One-Hot Encoded)")
plt.tight_layout()
plt.show()
```

### Feature Importance: Categorical zipCode


```{python}
#| label: importance-plot-cat-python
#| echo: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance for categorical zipCode
plt.figure(figsize=(8, 5))
plt.barh(range(len(importance_cat_df)), importance_cat_df['Importance'], 
         color='darkgreen', alpha=0.7)
plt.yticks(range(len(importance_cat_df)), importance_cat_df['Feature'])
plt.xlabel('Importance Score')
plt.title('Feature Importance (zipCode One-Hot Encoded)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```


## Discussion Questions and Answers for Challenge

### Answer to Question 1: Numerical vs Categorical Encoding

**How should zip code be modeled?**

- Zip code should be modeled as a categorical variable, not numerical
- Although it looks like a number, zip code represents neighborhood identityâ€”not a measurable quantity
- The difference between 50011 and 50012 has no economic meaning for house prices

**Is zip code ordinal or non-ordinal?**

- Zip code is non-ordinalâ€”there's no meaningful order among zip codes that helps predict house prices
- Unlike age, income, or square footage where higher values naturally mean more, zip codes don't work that way
- When we treat zip code as a number, the model assumes false ordering (like zip code 50013 is "greater than" 50012), which makes no sense for neighborhoods

**Why categorical variables need special treatment**

- When treated numerically, decision trees create meaningless splits like "Is zip code less than 50011.5?"
- These comparisons don't make sense for neighborhoods
- Feature importance gets distorted because it's based on artificial numeric thresholds rather than real location differences
- With categorical encoding (one-hot), the tree asks meaningful questions like "Is this property in zip code 50012?"
- This produces valid splits and trustworthy feature importance scores

**What my outputs showed**

- When I treated zip code as numeric, it had almost zero importance (around 0.00%)
- This isn't telling us neighborhoods don't matterâ€”it's telling us the encoding was wrong
- When I switched to categorical encoding, each zip code dummy variable had small but real importance
- This showed me that structural features (like square footage and garage size) dominate price prediction, but at least now the model evaluates neighborhoods correctly

---

### Answer to Question 2: R vs Python Implementation Differences

**Why the output tree and feature importance differ between R and Python**

- R and Python handle categorical variables very differently
- R's `rpart` package natively understands categorical variables (what R calls "factors")
- When you pass a categorical variable to `rpart` in R, the algorithm can directly work with the categories
- Python's scikit-learn doesn't support categorical variables at allâ€”everything has to be a number
- This forced me to use one-hot encoding as a workaround

**What R offers that Python does not**

- R can learn grouped splitsâ€”for example, putting zip codes {50010, 50012, 50014} in one group versus {50011, 50013, 50015} in another, all in a single node
- Python, with one-hot encoding, can only split on individual dummy variables, one at a time
- Zip code stays as one conceptual feature in R, but gets split across 25+ binary features in Python
- R keeps "neighborhood" as a single meaningful feature with cleaner feature importance

**Limitations of Python (supported by official documentation)**

From the [scikit-learn Decision Tree documentation](https://scikit-learn.org/stable/modules/tree.html):

> "The implementation of trees does not currently support categorical variables: they need to be encoded as integers or floats. As a result, categorical features are split based on their integer encoding, which often results in suboptimal splits."

This directly explains what I observedâ€”sklearn treats everything as numerical, which leads to suboptimal splits for categorical data.

**Which language models zip code better?**

- I think R does a better job modeling zip code as a categorical variable
- R treats it as a true categorical, allows multi-level splits, and keeps "neighborhood" as a single meaningful feature
- The feature importance comes out cleaner and easier to interpret
- Python works, but it's more of a workaroundâ€”I had to create 25+ binary features through one-hot encoding
- This splits zip code importance across all those dummy variables, making the tree wider, harder to interpret, and diluting the zip code importance score

---

### Answer to Question 3: Better Categorical Handling in Python

Even though sklearn's basic `DecisionTreeRegressor` doesn't support categorical variables, there are better options in Python that handle categories more intelligently than simple one-hot encoding.

**Option 1: HistGradientBoosting (scikit-learn)**

- Tree-based model from scikit-learn (not a single tree, but an ensemble) that can understand categorical features
- From the [scikit-learn documentation](https://scikit-learn.org/stable/modules/ensemble.html#categorical-support): "HistGradientBoostingClassifier and HistGradientBoostingRegressor have native support for categorical features: they can consider splits on non-ordered, categorical data."
- If you're willing to use a boosted tree model instead of a single tree, this is the official scikit-learn way to get proper categorical handling

**Option 2: CatBoost**

- Python library built specifically for handling categorical variables
- The [CatBoost documentation](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html) explicitly warns: "Do not use one-hot encoding during preprocessing. This affects both the training speed and the resulting quality."
- You just pass zip codes directly as strings or integers, and it transforms them internally without creating all those dummy columns

**Option 3: LightGBM**

- Another library that handles categoricals natively
- From the [LightGBM documentation](https://lightgbm.readthedocs.io/en/latest/Features.html): "It is common to represent categorical features with one-hot encoding, but this approach is suboptimal for tree learnersâ€¦ Instead of one-hot encoding, the optimal solution is to split on a categorical feature by partitioning its categories into 2 subsets."
- You mark columns as categorical, and LightGBM finds optimal splits internally

**Summary**

- If you want proper categorical handling in Python without one-hot encoding, step beyond plain `DecisionTreeRegressor`
- Use one of these modern tree-based methods: `HistGradientBoosting`, `CatBoost`, or `LightGBM`
- They all learn smarter splits directly on categorical values instead of treating them like numbers

---
```{python}
#| label: state-of-the-art-python
#| echo: false
#| warning: false

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
import warnings
warnings.filterwarnings('ignore')
warnings.simplefilter('ignore')
import os
os.environ['PYTHONWARNINGS'] = 'ignore'

# ---------------------------------------------------------
# 0. Load data and define features
# ---------------------------------------------------------
df = pd.read_csv("salesPriceData.csv")

model_vars = [
    'SalePrice', 'LotArea', 'YearBuilt', 'GrLivArea', 'FullBath',
    'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'zipCode'
]

df = df[model_vars].dropna()

feature_cols = [
    'LotArea', 'YearBuilt', 'GrLivArea', 'FullBath',
    'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'zipCode'
]

X = df[feature_cols].copy()
y = df['SalePrice'].copy()

# Make sure zipCode is treated as categorical
X['zipCode'] = X['zipCode'].astype(str)


# ---------------------------------------------------------
# 1. HistGradientBoostingRegressor (sklearn, native categorical)
# ---------------------------------------------------------
# HistGradientBoostingRegressor (sklearn, native categorical)
from sklearn.experimental import enable_hist_gradient_boosting  # noqa: F401
from sklearn.ensemble import HistGradientBoostingRegressor

X_hgb = X.copy()
X_hgb['zipCode'] = X_hgb['zipCode'].astype('category')
X_hgb['zipCode_code'] = X_hgb['zipCode'].cat.codes
X_hgb = X_hgb.drop(columns=['zipCode'])

cat_features_hgb = [X_hgb.columns.get_loc('zipCode_code')]

X_train_hgb, X_test_hgb, y_train_hgb, y_test_hgb = train_test_split(
    X_hgb, y, test_size=0.3, random_state=42
)

hgb = HistGradientBoostingRegressor(
    max_depth=6,
    learning_rate=0.1,
    max_iter=200,
    categorical_features=cat_features_hgb,
    random_state=42
)

hgb.fit(X_train_hgb, y_train_hgb)
y_pred_hgb = hgb.predict(X_test_hgb)
r2_hgb = r2_score(y_test_hgb, y_pred_hgb)


# ---------------------------------------------------------
# 2. CatBoostRegressor (native categorical handling)
# ---------------------------------------------------------
# CatBoostRegressor (native categorical)
from catboost import CatBoostRegressor, Pool

X_cb = X.copy()  # zipCode is string â†’ categorical
X_train_cb, X_test_cb, y_train_cb, y_test_cb = train_test_split(
    X_cb, y, test_size=0.3, random_state=42
)

# CatBoost expects indices of categorical columns
cat_features_cb = [X_cb.columns.get_loc('zipCode')]

train_pool = Pool(X_train_cb, y_train_cb, cat_features=cat_features_cb)
test_pool = Pool(X_test_cb, y_test_cb, cat_features=cat_features_cb)

cb = CatBoostRegressor(
    depth=6,
    learning_rate=0.1,
    iterations=300,
    loss_function='RMSE',
    verbose=False,
    random_state=42
)

cb.fit(train_pool, eval_set=test_pool)
y_pred_cb = cb.predict(test_pool)
r2_cb = r2_score(y_test_cb, y_pred_cb)


# ---------------------------------------------------------
# 3. LightGBM (LGBMRegressor with categorical_feature)
# ---------------------------------------------------------
# LightGBM (LGBMRegressor with categorical_feature)
import lightgbm as lgb

X_lgb = X.copy()
# LightGBM works well with category dtype / integer codes
X_lgb['zipCode'] = X_lgb['zipCode'].astype('category')

X_train_lgb, X_test_lgb, y_train_lgb, y_test_lgb = train_test_split(
    X_lgb, y, test_size=0.3, random_state=42
)

# Categorical feature names for LGBMRegressor
cat_features_lgb = ['zipCode']

lgbm = lgb.LGBMRegressor(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=-1,
    random_state=42,
    verbosity=-1,
    silent=True
)

lgbm.fit(
    X_train_lgb, y_train_lgb,
    categorical_feature=cat_features_lgb
)

y_pred_lgb = lgbm.predict(X_test_lgb)
r2_lgb = r2_score(y_test_lgb, y_pred_lgb)


# ---------------------------------------------------------
# Summary comparison
# ---------------------------------------------------------
print("\n=== Summary of R^2 scores ===")
print("HistGradientBoostingRegressor:", round(r2_hgb, 3))
print("CatBoostRegressor           :", round(r2_cb, 3))
print("LightGBM LGBMRegressor      :", round(r2_lgb, 3))

```

## Submission Checklist âœ…

**Minimum Requirements (Required for Any Points):**

- [ ] Forked starter repository from [https://github.com/flyaflya/decTreeChallenge.git](https://github.com/flyaflya/decTreeChallenge.git)
- [ ] Cloned repository locally using Cursor (or VS Code)
- [ ] Added thoughtful narrative answers to both discussion questions
- [ ] Document rendered to HTML successfully
- [ ] HTML files uploaded to your forked repository
- [ ] GitHub Pages enabled and working
- [ ] Site accessible at `https://[your-username].github.io/decTreeChallenge/`

**75% Grade Requirements:**

- [ ] Clear, well-reasoned answer to question 1 about numerical vs categorical encoding

**85% Grade Requirements:**

- [ ] Thorough analysis of question 2 with investigation of official documentation

**95% Grade Requirements:**

- [ ] Professional presentation style appropriate for business audience.
- [ ] Specific quote from official documentation of `sklearn.tree.DecisionTreeRegressor` supporting your analysis

**100% Grade Requirements:**

- [ ] Note on the current state of the art in decision tree implementation for categorical variables in Python.

**Report Quality (Critical for Higher Grades):**

- [ ] Clear, engaging narrative that tells a story
- [ ] Focus on the most interesting findings about decision tree feature importance
- [ ] Professional writing style (no AI-generated fluff)
- [ ] Concise analysis that gets to the point
- [ ] Practical insights that would help a real data scientist
- [ ] Documentation-based analysis for technical questions

